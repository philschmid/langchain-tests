{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U duckduckgo_search\n",
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beantworten Sie die Frage am Ende des Textes anhand der folgenden Informationen:\n",
      "### Eingabe\n",
      "Der DAX ist ein sogenannter Performance-Index und misst dabei den Wertzuwachs von Kapitalanlagen. Bei der Berechnung des Performance-Index werden Kapitalveränderungen und Ausschüttungen von...\n",
      "DAX (Kursindex) heute aktuell DAX (Kursindex)-Kurs in Realtime, DAX (Kursindex)-Chart, DAX (Kursindex) aktuell & live, Empfehlungen, gratis Analysen & Prognosen - boerse.de: Europas erstes ...\n",
      "Der Dax liegt zum Handelsschluss etwa 0,4 Prozent höher und notiert bei 15.655 Punkten. Mit dem heutigen Tageshoch in Form von 15.726 Punkten fehlten dem deutsche Leitindex nur elf Zähler, um...\n",
      "Frage: Wo steht der Dax?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import ddg\n",
    "\n",
    "keywords = 'Wo steht der Dax?'\n",
    "results = ddg(keywords, region='de', safesearch='On', time='m', max_results=3)\n",
    "\n",
    "prompt=f\"\"\"Beantworten Sie die Frage am Ende des Textes anhand der folgenden Informationen:\n",
    "### Eingabe\n",
    "{{context}}\n",
    "Frage: {{question}}\n",
    "\"\"\"\n",
    "\n",
    "formatted_prompt=prompt.format(context=\"\\n\".join([r[\"body\"] for r in results]), question=keywords)\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from duckduckgo_search import ddg\n",
    "\n",
    "class CustomSearchTool(BaseTool):\n",
    "    name = \"Search\"\n",
    "    description = \"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return ddg(query, region='en-Us', safesearch='On', time='y', max_results=3)\n",
    "    \n",
    "    async def _arun(self, query: str) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"Duckduck go does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import logging\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "class HuggingFacePipeline(LLM):\n",
    "    pipeline: Any  #: :meta private:\n",
    "    generation_kwargs: dict\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface_pipeline\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response = self.pipeline(prompt, **self.generation_kwargs)\n",
    "        if self.pipeline.task == \"text-generation\":\n",
    "            # Text generation return includes the starter text.\n",
    "            text = response[0][\"generated_text\"][len(prompt) :]\n",
    "        elif self.pipeline.task == \"text2text-generation\":\n",
    "            text = response[0][\"generated_text\"]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Got invalid task {self.pipeline.task}, \"\n",
    "            )\n",
    "        if stop is not None:\n",
    "            # This is a bit hacky, but I can't figure out a better way to enforce\n",
    "            # stop tokens when making calls to huggingface_hub.\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/envs/pytorch/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/pytorch/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006811618804931641,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fc434b927c4e01940a4f0cf3b7a5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_4269/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1540983265.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_4269/1540983265.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/langchain-tests/pydantic/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">341</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pydantic.main.BaseModel.__init__</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/home/ubuntu/langchain-tests/pydantic/main.py'</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValidationError: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> validation error for HuggingFacePipeline\n",
       "generate_kwargs\n",
       "  extra fields not permitted <span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #800080; text-decoration-color: #800080\">value_error</span>.extra<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_4269/\u001b[0m\u001b[1;33m1540983265.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_4269/1540983265.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ubuntu/langchain-tests/pydantic/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m341\u001b[0m in \u001b[92mpydantic.main.BaseModel.__init__\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/home/ubuntu/langchain-tests/pydantic/main.py'\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValidationError: \u001b[0m\u001b[1;36m1\u001b[0m validation error for HuggingFacePipeline\n",
       "generate_kwargs\n",
       "  extra fields not permitted \u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[35mvalue_error\u001b[0m.extra\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "model_id = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_kwargs = dict(\n",
    "    top_p=1,\n",
    "    top_k=0,\n",
    "    temperature=0.001,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=2048,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe,generation_kwargs=generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of February 2023, Joe Biden is the 46th and current President of the United States.\n"
     ]
    }
   ],
   "source": [
    "text = \"<|prompter|>Who is the president?<|endoftext|><|assistant|>\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|prompter|>Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, historical events, or other subjects. Input should be a search query.\n",
      "Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia, Search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "Question: {input}\n",
      "\n",
      "<|endoftext|><|assistant|>\n",
      "Thought:{agent_scratchpad}<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Anthropic\n",
    "from langchain.agents import load_tools, initialize_agent, Tool\n",
    "from langchain.tools import AIPluginTool\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "\n",
    "# TODO: doesn'T work\n",
    "PREFIX = \"\"\"<|prompter|>Answer the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "SUFFIX = \"\"\"Begin!\n",
    "Question: {input}\n",
    "\\n<|endoftext|><|assistant|>\n",
    "Thought:{agent_scratchpad}<|endoftext|>\"\"\"\n",
    "\n",
    "# tool = AIPluginTool.from_plugin_url(\"https://www.klarna.com/.well-known/ai-plugin.json\")\n",
    "tool = CustomSearchTool()\n",
    "tools = load_tools([\"wikipedia\"] )\n",
    "tools += [tool]\n",
    "\n",
    "agent_kwargs = {\"prefix\": PREFIX, \"suffix\": SUFFIX}\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, agent_kwargs=agent_kwargs\n",
    ")\n",
    "print(agent_chain.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = \"\"\"<|prompter|>Question: Who lived longer, Muhammad Ali or Alan Turing?<|endoftext|><|assistant|>\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\n",
    "<|prompter|>Question: When was the founder of craigslist born?<|endoftext|><|assistant|>\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\n",
    "<|prompter|>Question: Who was the maternal grandfather of George Washington?<|endoftext|><|assistant|>\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\n",
    "<|prompter|>Question: Are both the directors of Jaws and Casino Royale from the same country?<|endoftext|><|assistant|>\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Jaws?\n",
    "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
    "Follow up: Where is Steven Spielberg from?\n",
    "Intermediate answer: The United States.\n",
    "Follow up: Who is the director of Casino Royale?\n",
    "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
    "Follow up: Where is Martin Campbell from?\n",
    "Intermediate answer: New Zealand.\n",
    "So the final answer is: No\n",
    "\n",
    "<|prompter|>Question: {input}<|endoftext|><|assistant|>\n",
    "Are followup questions needed here:{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPER_API_KEY\"] = \"0605930b7bcc70497b8d0a6a37dad17ec000f837\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Anthropic\n",
    "from langchain.agents import load_tools, initialize_agent, Tool\n",
    "from langchain.tools import AIPluginTool\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "\n",
    "tool_names = [\"\"]\n",
    "# tool = load_tools([\"wikipedia\"])[0]\n",
    "# tool = CustomSearchTool()\n",
    "\n",
    "tool = load_tools([\"google-serper\"])[0]\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=tool.run,\n",
    "        description=tool.description,\n",
    "    )\n",
    "]\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True\n",
    ")\n",
    "agent_chain.agent.llm_chain.prompt.template = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: Who was the first German female canchellor?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mMerkel was the first female chancellor of Germany. During her tenure as Chancellor, Merkel was frequently referred to as the de facto leader of the European Union (EU), the most powerful woman in the world, and since 2016 the leader of the free world.\u001b[0m\u001b[32;1m\u001b[1;3mFollow up: What was Merkel's political career prior to becoming Chancellor?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mScientific career She obtained a doctorate in quantum chemistry in 1986 and worked as a research scientist until 1989. Merkel entered politics in the wake of the Revolutions of 1989, briefly serving as deputy spokeswoman for the first democratically elected Government of East Germany led by Lothar de Maizière.\u001b[0m\u001b[32;1m\u001b[1;3mFollow up: What was Merkel's political career after becoming Chancellor?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mA member of the Christian Democratic Union (CDU), she previously served as Leader of the Opposition from 2002 to 2005 and as Leader of the Christian Democratic Union from 2000 to 2018. Merkel was the first female chancellor of Germany.\u001b[0m\u001b[32;1m\u001b[1;3mSo the final answer is: Merkel\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Merkel'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"Who was the first German female canchellor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first German chancellor was Otto von Bismarck, who served from 1871 to 1890.'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"<|prompter|>Who was the first German chancellor?<|endoftext|><|assistant|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
