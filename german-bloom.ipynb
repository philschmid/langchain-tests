{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00699615478515625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61128e43c9074ef4911c2ddf9b732d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib.util\n",
    "import logging\n",
    "from typing import Any, List, Mapping, Optional\n",
    "import torch\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "class HuggingFacePipeline(LLM):\n",
    "    pipeline: Any  #: :meta private:\n",
    "    generation_kwargs: dict\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface_pipeline\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response = self.pipeline(prompt, **self.generation_kwargs)\n",
    "        if self.pipeline.task == \"text-generation\":\n",
    "            # Text generation return includes the starter text.\n",
    "            text = response[0][\"generated_text\"][len(prompt) :]\n",
    "        elif self.pipeline.task == \"text2text-generation\":\n",
    "            text = response[0][\"generated_text\"]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Got invalid task {self.pipeline.task}, \"\n",
    "            )\n",
    "        if stop is not None:\n",
    "            # This is a bit hacky, but I can't figure out a better way to enforce\n",
    "            # stop tokens when making calls to huggingface_hub.\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "        return text\n",
    "      \n",
    "\n",
    "    \n",
    "model_id = \"philschmid/instruct-igel-001\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "generate_kwargs = dict(\n",
    "    top_p=1,\n",
    "    top_k=0,\n",
    "    temperature=0.4,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=2048,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe,generation_kwargs=generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Heute ist das Wetter in der Stadt sonnig und warm bei etwa 30 °C.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"### Anweisung:\\n{input}\\n\\n### Antwort:\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "def generate(input_variables: Mapping[str, str],prompt=prompt) -> str:\n",
    "    return llm(prompt.format(**input_variables)).strip()\n",
    "\n",
    "\n",
    "generate({\"input\": \"Wie ist das Wetter heute?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort: Olaf Scholz.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "qa_template = \"\"\"Beantworten Sie die Frage am Ende des Textes anhand der folgenden Informationen. Wenn Sie die Antwort nicht wissen, sagen Sie einfach, dass Sie es nicht wissen, versuchen Sie nicht, eine Antwort zu erfinden.\n",
    "\n",
    "{context}\n",
    "\n",
    "Frage: {question}\"\"\"\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=qa_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "r = generate({\"context\": \"Olaf Scholz ist aktueller Bundeskanzler\", \"question\": \"Wer ist aktueller Kanzler?\"}, prompt=qa_prompt)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "os.environ[\"SERPER_API_KEY\"] = \"0605930b7bcc70497b8d0a6a37dad17ec000f837\"\n",
    "\n",
    "search = GoogleSerperAPIWrapper(k=3,gl=\"de\",hl=\"de\")\n",
    "\n",
    "# search.run(\"Wie ist das Wetter in Berlin?\")\n",
    "# print(search.run(\"Wo steht der Dax?\"))\n",
    "\n",
    "def google_search(input):\n",
    "  r = search.run(input)\n",
    "  # print(qa_prompt.format(context=r, question=input))\n",
    "  o = generate({\"context\": r, \"question\": input}, prompt=qa_prompt)\n",
    "  if \"Antwort:\" in o:\n",
    "    o = o.split(\"Antwort:\")[-1]\n",
    "  return o\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antworten:\n",
      "\n",
      "- Seit Juli 2017 ist Dr. Alfred Kranstedt Direktor des ITZBund.\n",
      "\n",
      "- Im November 2020 wurde Dr. Kranstedt in den Vorstand von NExT – Netzwerk: Experten für die digitale Transformation der Verwaltung, Direktorium, gewählt.\n",
      "\n",
      "- Dr. Alfred Kranstedt, Jahrgang 1966; Diplom-Informatiker (Universität Bielefeld) ; Direktor des Kaufmännischen Vizedirektors des ITZBund.\n",
      "\n",
      "- Seit Juli 2017 Direktor des ITZBund\n",
      "\n",
      "- Seit November 2020 Vorstandsmitglied bei NExT – Netzwerk: Experten für die digitale Transformation der Verwaltung, Direktorium, gewählt.\n",
      "\n",
      "- Dr. Alfred Kranstedt, Jahrgang 1966; Diplom-Informatiker (Universität Bielefeld) ; Direktor des Kaufmännischen Vizedirektors des ITZBund.\n"
     ]
    }
   ],
   "source": [
    "print(google_search(\"Wer ist aktueller Direktor des ITZBund?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antworten:\n",
      "\n",
      "1. Grunzen\n",
      "2. Krähen\n",
      "3. Knurren\n",
      "4. Grunzen\n",
      "5. Schnauben\n",
      "6. Grunzen\n",
      "7. Grunzen\n",
      "8. Grunzen\n",
      "9. Grunzen\n",
      "10. Grunzen\n",
      "11. Grunzen\n",
      "12. Grunzen\n",
      "13. Grunzen\n",
      "14. Grunzen\n",
      "15. Grunzen\n",
      "16. Grunzen\n",
      "17. Grunzen\n",
      "18. Grunzen\n",
      "19. Grunzen\n"
     ]
    }
   ],
   "source": [
    "print(google_search(\"Wie macht ein Schwein?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
