{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ = {**os.environ, **{env.split(\"=\")[0]: env.split(\"=\")[1] for env in open(\".env\", \"r\").readlines()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"\"\"You are an AI assistant for the Philipp Schmids blog. The blog is located at https://philschmid.de.\n",
    "You are given the following extracted parts of a long document and a question. Provide a conversational answer to the question.\n",
    "You should only use hyperlinks that are explicitly listed as a source in the context. Do NOT make up a hyperlink that is not listed.\n",
    "If the question includes a request for code, provide a code block directly from the blog.\n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "Question: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Answer in Markdown:\"\"\"\n",
    "\n",
    "question = \"Is PyTorch 2.0 backward compatible?\"\n",
    "context = \"\"\"n December 2, 2022, the PyTorch Team announced [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/) at the PyTorch Conference, focused on better performance, being faster, more pythonic, and staying as dynamic as before.\n",
    "\n",
    "This blog post explains how to get started with PyTorch 2.0 and Hugging Face Transformers today. It will cover how to fine-tune a BERT model for Text Classification using the newest PyTorch 2.0 features.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup environment & install Pytorch 2.0](#1-setup-environment--install-pytorch-20)\n",
    "2. [Load and prepare the dataset](#2-load-and-prepare-the-dataset)\n",
    "3. [Fine-tune & evaluate BERT model with the Hugging Face `Trainer`](#3-fine-tune--evaluate-bert-model-with-the-hugging-face-trainer)\n",
    "4. [Run Inference & test model](#4-run-inference--test-model)\n",
    "\n",
    "Before we can start, make sure you have a [Hugging Face Account](https://huggingface.co/join) to save artifacts and experiments.\n",
    "\n",
    "## Quick intro: Pytorch 2.0\n",
    "\n",
    "PyTorch 2.0 or, better, 1.14 is entirely backward compatible. Pytorch 2.0 will not require any modification to existing PyTorch code but can optimize your code by adding a single line of code with `model = torch.compile(model)`.\n",
    "If you ask yourself, why is there a new major version and no breaking changes? The PyTorch team answered this question in their [FAQ](https://pytorch.org/get-started/pytorch-2.0/#faqs): _â€œWe were releasing substantial new features that we believe change how you meaningfully use PyTorch, so we are calling it 2.0 instead.â€_\n",
    "\n",
    "Those new features include top-level support for TorchDynamo, AOTAutograd, PrimTorch, and TorchInductor.\n",
    "\n",
    "This allows PyTorch 2.0 to achieve a 1.3x-2x training time speedups supporting [today's 46 model architectures](https://github.com/pytorch/torchdynamo/issues/681) from [HuggingFace Transformers](https://github.com/huggingface/transformers)\n",
    "\n",
    "If you want to learn more about PyTorch 2.0, check out the official [â€œGET STARTEDâ€](https://pytorch.org/get-started/pytorch-2.0/).\n",
    "\n",
    "---\n",
    "\n",
    "Now we know how PyTorch 2.0 works, let's get started. ðŸš€\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "\n",
    "# load dataset and create list of string with page content\n",
    "dataset_id = \"philschmid/philschmid-de-blog\"\n",
    "ds = load_dataset(dataset_id)[\"train\"]\n",
    "\n",
    "print(f\"dataset length: {len(ds)}\")\n",
    "print(ds[randint(0, len(ds))].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from random import randint\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text_splitter = MarkdownTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer, chunk_size=500, chunk_overlap=12)\n",
    "texts = text_splitter.create_documents(ds[\"content\"],metadatas=[{\"source\": s[\"url\"], \"title\": s[\"title\"],\"date\": s[\"date\"], \"tags\": s[\"tags\"] } for s in ds])\n",
    "\n",
    "\n",
    "print(f\"dataset length: {len(texts)}\")\n",
    "print(texts[randint(0, len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# create vector store\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "\n",
    "# create vector store and save it to disk\n",
    "# db = FAISS.from_documents(texts, embeddings)\n",
    "# db.save_local(\"faiss_index\")\n",
    "\n",
    "# # load from disk \n",
    "db = FAISS.load_local(\"faiss_index\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.llms import HuggingFaceHub, OpenAI,Anthropic\n",
    "\n",
    "# llm = HuggingFaceHub(repo_id=\"OpenAssistant/oasst-sft-1-pythia-12b\", huggingfacehub_api_token=os.environ[\"HF_API_KEY\"])\n",
    "# llm = OpenAI()\n",
    "import os \n",
    "\n",
    "llm = Anthropic(top_p=0.9, temperature=1)\n",
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=llm,\n",
    "                                chain_type=\"stuff\", \n",
    "                                vectorstore=db, \n",
    "                                return_source_documents=True,\n",
    "                                k=2\n",
    ")\n",
    "\n",
    "# def query_blog(query:str , model=\"t5\"):\n",
    "#   if model == \"t5\":\n",
    "#     llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", huggingfacehub_api_token=os.environ[\"HF_API_KEY\"])\n",
    "#     print(\"using HuggingFaceHub model: google/flan-t5-xxl\")\n",
    "#   elif model == \"ul2\":\n",
    "#     llm = HuggingFaceHub(repo_id=\"google/flan-ul2\", huggingfacehub_api_token=os.environ[\"HF_API_KEY\"])\n",
    "#     print(\"using HuggingFaceHub model: google/flan-ul2\")\n",
    "#   elif model == \"oa\":\n",
    "#     llm = HuggingFaceHub(repo_id=\"OpenAssistant/oasst-sft-1-pythia-12b\", huggingfacehub_api_token=os.environ[\"HF_API_KEY\"])\n",
    "#     print(\"using HuggingFaceHub model: OpenAssistant/oasst-sft-1-pythia-12b\")\n",
    "#   else: \n",
    "#     llm = OpenAI()\n",
    "#     print(\"using OpenAI model\")\n",
    "#   qa.combine_documents_chain.llm_chain.llm = llm\n",
    "  \n",
    "#   res = qa({\"query\": query})\n",
    "#   answer = res[\"result\"].strip()\n",
    "#   ref = \",\".join([ f\"[[{idx+1}]]({url})\" for idx, url in enumerate(set([r.metadata[\"url\"] for r in res[\"source_documents\"]]))])\n",
    "#   display(Markdown(answer + \" \" + ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to use Hugging Face Transformers with PyTorch 2.0\"\n",
    "res = qa({\"query\": query})\n",
    "\n",
    "# flan t5\n",
    "# query_blog(query, model=\"t5\")\n",
    "# # flan ul2\n",
    "# query_blog(query, model=\"ul2\")\n",
    "# openai\n",
    "# query_blog(query, model=\"oa\")\n",
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"\"\"1. Setup environment & install Pytorch 2.0\\n\\nOur first step is to install PyTorch 2.0 and the Hugging Face Libraries, including `transformers` and `datasets`.\\n\\n```python\\n# Install PyTorch 2.0 with cuda 11.7\\n!pip install \"torch>=2.0\" --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade --quiet\\n```\\n\\nAdditionally, we are installing the latest version of `transformers` from the `main` git branch, which includes the native integration of PyTorch 2.0 into the `Trainer`.\\n\\n```python\\n# Install transformers and dataset\\n!pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" tensorboard scikit-learn\\n# Install git-fls for pushing model and logs to the hugging face hub\\n!sudo apt-get install git-lfs --yes\\n```\\n\\nThis example will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. To push our model to the Hub, you must register on the [Hugging Face](https://huggingface.co/join). If you already have an account, you can skip this step. After you have an account, we will use the `login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk.\\n\\n```python\\nfrom huggingface_hub import login\\n\\nlogin(\\n  token=\"\", # ADD YOUR TOKEN HERE\\n  add_to_git_credential=True\\n)\\n\\n\"\"\"\n",
    "qa.combine_documents_chain.llm_chain.prompt.format(context=context,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# create vector store\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "llm = HuggingFaceHub(repo_id=\"OpenAssistant/oasst-sft-1-pythia-12b\", huggingfacehub_api_token=os.environ[\"HF_API_KEY\"], model_kwargs={\"do_sample\": True, \"max_new_tokens\": 512, \"top_p\":0.8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i deploy BERT?\"\n",
    "\n",
    "docs = db.similarity_search(query=query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "oa_prompt = PromptTemplate(\n",
    "  input_variables=[\"summaries\", \"question\"],\n",
    "  template=\"\"\"<|prompter|>Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{summaries}\\n\\nQuestion: {question}\\nHelpful Answer:<|endoftext|><|assistant|>\"\"\"\n",
    ")\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=oa_prompt)\n",
    "\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html#return-source-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# create vector store\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "llm = HuggingFaceHub(repo_id=\"OpenAssistant/oasst-sft-1-pythia-12b\", huggingfacehub_api_token=os.environ[\"HF_API_KEY\"], \n",
    "                     model_kwargs={\"do_sample\": True, \"max_new_tokens\": 512, \"top_p\":0.8})\n",
    "\n",
    "\n",
    "oa_prompt = PromptTemplate(\n",
    "  input_variables=[\"context\", \"question\"],\n",
    "  template=\"\"\"<|prompter|>Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:<|endoftext|><|assistant|>\"\"\"\n",
    ")\n",
    "\n",
    "# chain_type_kwargs = {\"prompt\": oa_prompt}\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "                                #  chain_type_kwargs=chain_type_kwargs,\n",
    "                                 return_source_documents=True\n",
    "                                 )\n",
    "qa.combine_documents_chain.llm_chain.prompt = oa_prompt\n",
    "\n",
    "\n",
    "\n",
    "query=\"How can deploy BERT with Amazon SageMaker?\"\n",
    "res = qa({\"query\": query})\n",
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "question_prompt_template = \"\"\"<|prompter|>Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
    "Return any relevant text.\n",
    "{context}\n",
    "Question: {question}\n",
    "Relevant text, if any:<|endoftext|><|assistant|>\"\"\"\n",
    "QUESTION_PROMPT = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "combine_prompt_template = \"\"\"<|prompter|>Given the following extracted parts of a long document and a question, create a final answer. \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "Helpful Answer:<|endoftext|><|assistant|>\"\"\"\n",
    "COMBINE_PROMPT = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "chain = load_qa_chain(llm, chain_type=\"map_reduce\", return_map_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\n",
    "\n",
    "query=\"How can deploy BERT with Amazon SageMaker?\"\n",
    "\n",
    "res = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "article=\"\"\"Hugging Face, a top supplier of open-source machine learning tools, and AWS have joined together to increase the access to artificial intelligence (AI). Hugging Face's cutting-edge transformers and natural language processing (NLP) models will be made available to AWS customers as a result of the cooperation, making it simpler for them to develop and deploy AI applications.\n",
    "\n",
    "Hugging Face has become well-known in the AI community for its free, open-source transformers library, which is used by thousands of programmers all around the world to build cutting-edge AI models for a range of tasks, including sentiment analysis, language translation, and text summarization. By partnering with AWS, Hugging Face will be able to provide its tools and expertise to a broader audience.\n",
    "\n",
    "Generative AI has the potential to transform entire industries, but its cost and the required expertise puts the technology out of reach for all but a select few companies, said Adam Selipsky, CEO of AWS. Hugging Face and AWS are making it easier for customers to access popular machine learning models to create their own generative AI applications with the highest performance and lowest costs. This partnership demonstrates how generative AI companies and AWS can work together to put this innovative technology into the hands of more customers.\n",
    "\n",
    "AWS has increased the scope of its own generative AI offerings. For instance, it improved the AWS QuickSight Q business projections to comprehend common phrases like \"show me a forecast.\" The Microsoft-owned GitHub Copilot, which uses models built from OpenAI's Codex, has competition in the form of AWS's Amazon CodeWhisperer, an AI programming assistant that autocompletes software code by extrapolating from a user's initial hints.\n",
    "\n",
    "As part of the collaboration, Hugging Face's models will be integrated with AWS services like Amazon SageMaker, a platform for creating, honing, and deploying machine learning models. This will make it simple for developers to create their own AI applications using pre-trained models from Hugging Face without needing to have considerable machine learning knowledge.\n",
    "\n",
    "The future of AI is here, but itâ€™s not evenly distributed, said Clement Delangue, CEO of Hugging Face.\"\"\"\n",
    "article2 = \"\"\"Accessibility and transparency are the keys to sharing progress and creating tools to use these new capabilities wisely and responsibly. Amazon SageMaker and AWS-designed chips will enable our team and the larger machine learning community to convert the latest research into openly reproducible models that anyone can build on.\n",
    "\n",
    "Hugging Face offers a library of over 10,000 Hugging Face Transformers models that you can run on Amazon SageMaker. With just a few lines of code, you can import, train, and fine-tune pre-trained NLP Transformers models such as BERT, GPT-2, RoBERTa, XLM, DistilBert, and deploy them on Amazon SageMaker. \n",
    "\n",
    "Hugging Face and AWS's partnership is anticipated to have a big impact on the AI market since it will make it possible for more companies and developers to use cutting-edge AI tools to generate unique consumer solutions.\"\"\"\n",
    "\n",
    "docs = [Document(page_content=article), Document(page_content=article2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-ul2\", huggingfacehub_api_token=os.environ[\"HF_API_KEY\"],\n",
    "                      model_kwargs={\"do_sample\": True, \"max_new_tokens\": 512, \"top_p\":0.8})\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", return_intermediate_steps=True)\n",
    "sum = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\", return_intermediate_steps=True, question_prompt=PROMPT, refine_prompt=refine_prompt)\n",
    "chain({\"input_documents\": docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.prompt_length(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id=\"OpenAssistant/oasst-sft-1-pythia-12b\", huggingfacehub_api_token=os.environ[\"HF_API_KEY\"], \n",
    "                     model_kwargs={\"do_sample\": True, \"max_new_tokens\": 512, \"top_p\":0.8})\n",
    "\n",
    "\n",
    "oa_prompt = PromptTemplate(\n",
    "  input_variables=[\"text\"],\n",
    "  template=\"\"\"<|prompter|>Write a concise summary of the following:\n",
    "\n",
    "\n",
    "{text}<|endoftext|><|assistant|>\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)# return_intermediate_steps=True)\n",
    "sum = chain({\"input_documents\": docs})\n",
    "sum[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of dictonary with random firstnames and lastnames\n",
    "\n",
    "user = [\n",
    "  { \"firstname\": \"John\", \"lastname\": \"Doe\"},\n",
    "  { \"firstname\": \"Jane\", \"lastname\": \"Doe\"},\n",
    "  { \"firstname\": \"John\", \"lastname\": \"Smith\"},\n",
    "  { \"firstname\": \"Jane\", \"lastname\": \"Smith\"},\n",
    "  ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
