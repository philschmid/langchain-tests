{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idea\n",
    "\n",
    "1. google search\n",
    "2. traifatura web page\n",
    "3. split web page in chunks \n",
    "4. embedd web page\n",
    "5. simiarity web page\n",
    "6. q&a llm \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ = {**os.environ, **{env.split(\"=\")[0]: env.split(\"=\")[1].strip() for env in open(\".env\", \"r\").readlines()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.base import Chain\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from trafilatura import fetch_url, extract\n",
    "from trafilatura.settings import ConfigParser, use_config\n",
    "from typing import Dict, List\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class SearchWebsiteChainWithEncoder(Chain):\n",
    "    search_engine = GoogleSearchAPIWrapper()\n",
    "    top_k_search_results = 3\n",
    "    top_k_documents = 5\n",
    "    website_search_timeout = 5\n",
    "    website_retries=2\n",
    "    website_language = \"en\"\n",
    "    tokenizer_id = \"bert-base-uncased\"\n",
    "    cross_encoder_model = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    \n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"query\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['documents']\n",
    "\n",
    "    # Pydantic doesn't support post_init hooks, so we need to do this manually\n",
    "    @property\n",
    "    def tokenizer(self) -> List[str]:\n",
    "        return AutoTokenizer.from_pretrained(self.tokenizer_id)\n",
    "\n",
    "    @property\n",
    "    def text_splitter(self) -> List[str]:\n",
    "        return CharacterTextSplitter.from_huggingface_tokenizer(self.tokenizer, separator = \"\\n\", chunk_size=100, chunk_overlap=12)\n",
    "\n",
    "    @property\n",
    "    def trafilatura_config(self) -> List[str]:\n",
    "        c = use_config()\n",
    "        c.set('DEFAULT', 'DOWNLOAD_TIMEOUT', str(self.website_search_timeout))\n",
    "        c.set('DEFAULT', 'USER_AGENTS', \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\")\n",
    "        return c\n",
    "\n",
    "    @property\n",
    "    def cross_encoder(self) -> CrossEncoder:\n",
    "        return CrossEncoder(self.cross_encoder_model)\n",
    "    \n",
    "    def get_html_data(self ,url):        \n",
    "        downloaded = fetch_url(url, config=self.trafilatura_config)\n",
    "        return extract(downloaded,target_language=self.website_language, include_comments=False, include_formatting=False, include_tables=False, with_metadata=False)\n",
    "\n",
    "    def split_content_into_documents(self, content: List[Dict[str,str]]) -> List[str]:\n",
    "        metadata = [{\"source\": s[\"link\"]} for s in content]\n",
    "        documents = [s[\"content\"] for s in content]\n",
    "        texts = self.text_splitter.create_documents(documents,metadatas=metadata)\n",
    "        return texts\n",
    "\n",
    "    def score_documents(self,query:str ,documents: List[Document]) -> List[Dict[str,str]]:\n",
    "        # So we create the respective sentence combinations\n",
    "        sentence_combinations = [[query, corpus_sentence.page_content] for corpus_sentence in documents]\n",
    "\n",
    "        # Compute the similarity scores for these combinations\n",
    "        similarity_scores = self.cross_encoder.predict(sentence_combinations)\n",
    "        # Sort the scores in decreasing order\n",
    "        sim_scores_argsort = reversed(np.argsort(similarity_scores))\n",
    "        # get top 2 results \n",
    "        top_results = []\n",
    "        for idx in list(sim_scores_argsort)[:self.top_k_documents]:\n",
    "            x = documents[idx]\n",
    "            x.metadata[\"score\"] = similarity_scores[idx]\n",
    "            top_results.append(x)\n",
    "        return top_results\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        # define search query\n",
    "        query = inputs.get(\"query\")\n",
    "        \n",
    "        # search using search engine\n",
    "        search_results = self.search_engine.results(query,num_results=self.top_k_search_results)\n",
    "        # extract html data from search results\n",
    "        results_with_content = []\n",
    "        for result in search_results:\n",
    "            html = self.get_html_data(result[\"link\"])\n",
    "            if html: \n",
    "                results_with_content.append({**result, **{\"content\": html}})\n",
    "        \n",
    "        # convert search results into documents\n",
    "        documents = self.split_content_into_documents(results_with_content)\n",
    "        \n",
    "        # score documents\n",
    "        scored_documents = self.score_documents(query,documents)\n",
    "        \n",
    "        # search.results(query,num_results=3)\n",
    "        # output_1 = self.chain_1.run(inputs)\n",
    "        # output_2 = self.chain_2.run(inputs)\n",
    "        return {'documents': scored_documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = SearchWebsiteChainWithEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "import os \n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", \n",
    "                     huggingfacehub_api_token=os.environ[\"HF_API_KEY\"], \n",
    "                     model_kwargs={\"do_sample\": True, \"max_new_tokens\": 512, \"top_p\":0.6, \"temperature\":0.6 })\n",
    "\n",
    "# llm = HuggingFaceHub(repo_id=\"google/flan-ul2\", \n",
    "#                      huggingfacehub_api_token=os.environ[\"HF_API_KEY\"], \n",
    "#                      model_kwargs={\"do_sample\": True, \"max_new_tokens\": 512, \"top_p\":0.9, \"temperature\":0.3 })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from typing import Any, Dict, List\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "oa_prompt = PromptTemplate(\n",
    "  input_variables=[\"context\", \"question\"],\n",
    "  template=\"<|prompter|>Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:<|endoftext|><|assistant|>\"\n",
    ")\n",
    "\n",
    "\n",
    "class LLMWebSearchChain(Chain):\n",
    "    llm: Any\n",
    "    websearch = SearchWebsiteChainWithEncoder()\n",
    "    prompt: PromptTemplate = oa_prompt\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"query\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['answer']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        search_result = self.websearch.run(inputs)\n",
    "        format_prompt = self.prompt.format(context=\"\\n\".join([p.page_content for p in search_result]), question=inputs[\"query\"])\n",
    "        \n",
    "        answer = self.llm(format_prompt)\n",
    "        return {'answer': answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = SearchWebsiteChainWithEncoder(top_k_search_results=2,top_k_documents=5)\n",
    "search = LLMWebSearchChain(llm=llm, websearch=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.llms import Anthropic\n",
    "\n",
    "\n",
    "# llm = OpenAI(temperature=0)\n",
    "# llm = Anthropic(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 115, which is longer than the specified 100\n",
      "Created a chunk of size 104, which is longer than the specified 100\n",
      "Created a chunk of size 106, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current stock market value of the DAX is around 10,9,000 points.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"What is the current stock market value of the dax?\"\n",
    "\n",
    "search.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 115, which is longer than the specified 100\n",
      "Created a chunk of size 104, which is longer than the specified 100\n",
      "Created a chunk of size 106, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The performance of the insurance sector was significantly better. The STOXX Europe 600 Insurance traded almost unchanged (-1.0%). The Allianz share recorded a slight fall of 3.3% to 200.90 euros. Including the dividend of 10.80 euros, the increase in value was 2.0%. In a ten-year comparison, the average annual increase in value was 11.8%.', metadata={'source': 'https://www.allianz.com/en/investor_relations/share/share-price.html', 'score': -6.3316245}),\n",
       " Document(page_content='It’s useful to look at stock market levels compared to where they’ve been over the past few months. When the S&P 500 is above its moving or rolling average of the prior 125 trading days, that’s a sign of positive momentum. But if the index is below this average, it shows investors are getting skittish. The Fear & Greed Index uses slowing momentum as a signal for Fear and a growing momentum for Greed.', metadata={'source': 'https://www.cnn.com/markets/fear-and-greed', 'score': -10.157162}),\n",
       " Document(page_content='Markets\\nFear & Greed Index\\nLatest Market News\\nS&P 500\\n125-day moving average', metadata={'source': 'https://www.cnn.com/markets/fear-and-greed', 'score': -10.175359}),\n",
       " Document(page_content='The market is made up of thousands of stocks. And on any given day, investors are actively buying and selling them. This measure looks at the amount, or volume, of shares on the NYSE that are rising compared to the number of shares that are falling. A low (or even negative) number is a bearish sign. The Fear & Greed Index uses decreasing trading volume as a signal for Fear.', metadata={'source': 'https://www.cnn.com/markets/fear-and-greed', 'score': -10.187151}),\n",
       " Document(page_content='The Fear & Greed Index is a compilation of seven different indicators that measure some aspect of stock market behavior. They are market momentum, stock price strength, stock price breadth, put and call options, junk bond demand, market volatility, and safe haven demand. The index tracks how much these individual indicators deviate from their averages compared to how much they normally diverge. The index gives each indicator equal weighting in calculating a score from 0 to 100, with 100 representing maximum greediness and 0 signaling maximum fear.', metadata={'source': 'https://www.cnn.com/markets/fear-and-greed', 'score': -10.280758})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current stock market value of the Dow Jones Industrial Average (DJIA) is around 26,000 points.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm(f\"<|prompter|>{query}<|endoftext|><|assistant|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
